{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c03b397d-fccc-46d2-aa4f-7451c74017df",
   "metadata": {},
   "source": [
    " # Imports and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a57cd3-94fe-453f-ae8e-2bd779ac9447",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import pymongo\n",
    "import tqdm.auto as tqdm\n",
    "import warnings\n",
    "\n",
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "spike_range = namedtuple(\"spike_range\", [\"min\",\"max\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ccfd74-6e86-48d4-aab6-c121c0262b85",
   "metadata": {},
   "source": [
    "# Connect to Mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d5faec-7155-4d06-8974-4f1cbe4ad38f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mongo = pymongo.MongoClient(\"\")['fingerprints']['training']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5ea766-c68c-437d-9de6-d6b1b515db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo = mongo['fingerprints']['training']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3cdbaeff-5979-4cfd-a8ad-ec08dcc2fc6f",
   "metadata": {},
   "source": [
    "## Defining our parameters:\n",
    "    Action Space: The list of actions that the agent takes to modify the enviroment.\n",
    "    State Space: The current filtered tensor torch.FloatTensor(df[[col for col in df.columns if col.startswith(\"spike\")]].to_numpy())\n",
    "    For state space, It should just be a singular tensor representing the PCP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3cc5f6-f69d-4e64-9e7f-635779ac680e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2df2d9c-9d03-4048-8b61-499992a67412",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FingerprintEnv(object):\n",
    "    def __init__(self, df):\n",
    "        super(FingerprintEnv, self).__init__()\n",
    "        self.total_df = df\n",
    "        self.df = self.nextBatch()\n",
    "        self.filtered_df = self.df.copy()\n",
    "        self.currentState = self.imgState()\n",
    "        self.stateSpace = [0]\n",
    "        self.filter = {\n",
    "            'spike0': spike_range(min=df['spike0'].min(), max=df['spike0'].max()),\n",
    "            'spike1': spike_range(min=df['spike1'].min(), max=df['spike1'].max()),\n",
    "            'spike2': spike_range(min=df['spike2'].min(), max=df['spike2'].max()),\n",
    "            'spike3': spike_range(min=df['spike3'].min(), max=df['spike3'].max()),\n",
    "            'spike4': spike_range(min=df['spike4'].min(), max=df['spike4'].max()),\n",
    "            'spike5': spike_range(min=df['spike5'].min(), max=df['spike5'].max()),\n",
    "            'spike6': spike_range(min=df['spike6'].min(), max=df['spike6'].max()),\n",
    "            'spike7': spike_range(min=df['spike7'].min(), max=df['spike7'].max()),\n",
    "            'spike8': spike_range(min=df['spike8'].min(), max=df['spike8'].max())\n",
    "        }\n",
    "        self.actionSpace = {\n",
    "            \"spike0-maxUP\" : self.increment_max,\n",
    "            \"spike0-maxDOWN\" : self.decrement_max,\n",
    "            \"spike0-minUP\" : self.increment_min,\n",
    "            \"spike0-minDOWN\" : self.decrement_min,\n",
    "            \"spike1-maxUP\" : self.increment_max,\n",
    "            \"spike1-maxDOWN\" : self.decrement_max,\n",
    "            \"spike1-minUP\" : self.increment_min,\n",
    "            \"spike1-minDOWN\" : self.decrement_min,\n",
    "            \"spike2-maxUP\" : self.increment_max,\n",
    "            \"spike2-maxDOWN\" : self.decrement_max,\n",
    "            \"spike2-minUP\" : self.increment_min,\n",
    "            \"spike2-minDOWN\" : self.decrement_min,\n",
    "            \"spike3-maxUP\" : self.increment_max,\n",
    "            \"spike3-maxDOWN\" : self.decrement_max,\n",
    "            \"spike3-minUP\" : self.increment_min,\n",
    "            \"spike3-minDOWN\" : self.decrement_min,\n",
    "            \"spike4-maxUP\" : self.increment_max,\n",
    "            \"spike4-maxDOWN\" : self.decrement_max,\n",
    "            \"spike4-minUP\" : self.increment_min,\n",
    "            \"spike4-minDOWN\" : self.decrement_min,\n",
    "            \"spike5-maxUP\" : self.increment_max,\n",
    "            \"spike5-maxDOWN\" : self.decrement_max,\n",
    "            \"spike5-minUP\" : self.increment_min,\n",
    "            \"spike5-minDOWN\" : self.decrement_min,\n",
    "            \"spike6-maxUP\" : self.increment_max,\n",
    "            \"spike6-maxDOWN\" : self.decrement_max,\n",
    "            \"spike6-minUP\" : self.increment_min,\n",
    "            \"spike6-minDOWN\" : self.decrement_min,\n",
    "            \"spike7-maxUP\" : self.increment_max,\n",
    "            \"spike7-maxDOWN\" : self.decrement_max,\n",
    "            \"spike7-minUP\" : self.increment_min,\n",
    "            \"spike7-minDOWN\" : self.decrement_min,\n",
    "            \"spike8-maxUP\" : self.increment_max,\n",
    "            \"spike8-maxDOWN\" : self.decrement_max,\n",
    "            \"spike8-minUP\" : self.increment_min,\n",
    "            \"spike8-minDOWN\" : self.decrement_min,\n",
    "            \"noaction\" : None\n",
    "        }\n",
    "        self.possibleActions = list(self.actionSpace.keys())\n",
    "    def nextBatch(self):\n",
    "        sample = pd.Series(self.total_df.moniker.unique()).sample(10).unique().tolist()\n",
    "        df = self.total_df[self.total_df['moniker'].isin(sample)]\n",
    "        for key, val in df.moniker.value_counts().items():\n",
    "            if val < 5:\n",
    "                df = df[df['moniker']!=key]\n",
    "        return df\n",
    "    \n",
    "    def increment_max(self, spike):\n",
    "        spikeVal = self.filter[spike]\n",
    "        self.filter[spike] = spike_range(spikeVal[0], spikeVal[1] + .5)\n",
    "        return\n",
    "    \n",
    "    def decrement_max(self, spike):\n",
    "        spikeVal = self.filter[spike]\n",
    "        if spikeVal[1]-.5 >= spikeVal[0]:\n",
    "            self.filter[spike] = spike_range(spikeVal[0], spikeVal[1] - .5)\n",
    "        return\n",
    "    \n",
    "    def increment_min(self, spike):\n",
    "        spikeVal = self.filter[spike]\n",
    "        if spikeVal[0]+.5 <= spikeVal[1]:\n",
    "            self.filter[spike] = spike_range(spikeVal[0] + .5, spikeVal[1])\n",
    "        return\n",
    "    \n",
    "    def decrement_min(self, spike):\n",
    "        spikeVal = self.filter[spike]\n",
    "        self.filter[spike] = spike_range(spikeVal[0] - .5, spikeVal[1])\n",
    "        return\n",
    "    \n",
    "    def apply_filters(self):\n",
    "        self.filtered_df = self.df.copy()\n",
    "        for key,value in self.filter.items():\n",
    "            self.filtered_df = self.filtered_df[self.filtered_df[key].between(value[0], value[1])]\n",
    "        return\n",
    "    \n",
    "    def imgState(self):\n",
    "        img = Image.open(\n",
    "            io.BytesIO(\n",
    "                px.parallel_coordinates(\n",
    "                    self.filtered_df[[col for col in self.filtered_df.columns if col.startswith(\"spike\")]],\n",
    "                    height = 256,\n",
    "                    width = 256, \n",
    "                ).to_image(width=384, height=384)\n",
    "            )\n",
    "        )\n",
    "        img = transforms.functional.crop(img, top=60, left=60, height=324, width=324)\n",
    "        img = transforms.functional.rotate(img, 180)\n",
    "        img = transforms.functional.crop(img, top=68, left=68, height=256, width=256)\n",
    "        img = transforms.functional.rotate(img, 180)\n",
    "        img = transforms.functional.to_tensor(img)\n",
    "        img = transforms.functional.rgb_to_grayscale(img[:3], 1)\n",
    "        return img\n",
    "    \n",
    "    def get_reward(self, action):\n",
    "        monikers = self.filtered_df.moniker.unique().tolist()\n",
    "        total_possible = self.df[self.df.moniker.isin(monikers)].shape[0] # Grab the number of records for all monikers in total\n",
    "        current_total = self.filtered_df[self.filtered_df.moniker.isin(monikers)].shape[0] # Grab the number of records for all monikers within current filter\n",
    "        num_monikers = len(monikers)\n",
    "        if num_monikers == 0:\n",
    "            num_monikers = 10000\n",
    "        return (current_total/(num_monikers) - total_possible)\n",
    "    \n",
    "    def step(self, action):\n",
    "        ## Take the action\n",
    "        if action != \"noaction\":\n",
    "            self.actionSpace[action](action.split(\"-\")[0])\n",
    "            \n",
    "        ## Filter the data\n",
    "        self.apply_filters()\n",
    "        self.currentState = self.imgState()\n",
    "        reward = self.get_reward(action)\n",
    "        return self.currentState, reward, reward==0, None\n",
    "    \n",
    "    def moniker_progress(self):\n",
    "        found = self.filtered_df.moniker.value_counts().to_dict()\n",
    "        total = self.df.moniker.value_counts().to_dict()\n",
    "        for k,v in found.items():\n",
    "            found[f\"{k}_total\"] = total[k]\n",
    "        return found\n",
    "\n",
    "    def reset(self):\n",
    "        self.df = self.nextBatch()\n",
    "        self.filtered_df = self.df.copy()\n",
    "        self.filter = {\n",
    "            'spike0': spike_range(min=self.df['spike0'].min(), max=self.df['spike0'].max()),\n",
    "            'spike1': spike_range(min=self.df['spike1'].min(), max=self.df['spike1'].max()),\n",
    "            'spike2': spike_range(min=self.df['spike2'].min(), max=self.df['spike2'].max()),\n",
    "            'spike3': spike_range(min=self.df['spike3'].min(), max=self.df['spike3'].max()),\n",
    "            'spike4': spike_range(min=self.df['spike4'].min(), max=self.df['spike4'].max()),\n",
    "            'spike5': spike_range(min=self.df['spike5'].min(), max=self.df['spike5'].max()),\n",
    "            'spike6': spike_range(min=self.df['spike6'].min(), max=self.df['spike6'].max()),\n",
    "            'spike7': spike_range(min=self.df['spike7'].min(), max=self.df['spike7'].max()),\n",
    "            'spike8': spike_range(min=self.df['spike8'].min(), max=self.df['spike8'].max())\n",
    "        }\n",
    "        self.currentState = self.imgState()\n",
    "        return self.currentState\n",
    "    \n",
    "    def render(self):\n",
    "        img = transforms.functional.to_pil_image(self.currentState)\n",
    "        display(img)\n",
    "        # fig,ax = plt.subplots(figsize=(28,28))\n",
    "        # ax.imshow(img)\n",
    "        # plt.show()\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc240098-78a8-460c-bfb3-c7e340bb719e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7f191b-739c-4142-8add-4d882fe5893f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, ALPHA):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        #self.conv1 = nn.Conv2d(3, 32, 8, stride=4, padding=1)\n",
    "        self.conv1 = nn.Conv2d(1, 32, 8, stride=4, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "        #self.fc1 = nn.Linear(128*23*16, 512)\n",
    "        self.fc1 = nn.LazyLinear(512)\n",
    "        self.fc2 = nn.Linear(512, 37)\n",
    "        #self.optimizer = optim.SGD(self.parameters(), lr=self.ALPHA, momentum=0.9)\n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr=ALPHA)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, observation):\n",
    "        observation = T.Tensor(observation).to(self.device)\n",
    "        #observation = observation.view(-1, 3, 210, 160).to(self.device)\n",
    "        observation = observation.view(-1, 1, 256, 256)\n",
    "        observation = F.relu(self.conv1(observation))\n",
    "        observation = F.relu(self.conv2(observation))\n",
    "        observation = F.relu(self.conv3(observation))\n",
    "        #observation = observation.view(-1, 128*23*16).to(self.device)\n",
    "        observation = observation.view(-1, 128*28*28)\n",
    "        observation = F.relu(self.fc1(observation))\n",
    "        actions = self.fc2(observation)\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed20b887-76c1-4bd2-ab15-2f739e75fc93",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57c7a1d-3096-4182-9401-8368f4616e3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, gamma, epsilon, alpha,\n",
    "                 maxMemorySize, epsEnd=0.05,\n",
    "                 replace=10000, actionSpace=[0,1,2,3,4,5]):\n",
    "        self.GAMMA = gamma\n",
    "        self.EPSILON = epsilon\n",
    "        self.EPS_END = epsEnd\n",
    "        self.ALPHA = alpha\n",
    "        self.actionSpace = actionSpace\n",
    "        self.memSize = maxMemorySize\n",
    "        self.steps = 0\n",
    "        self.learn_step_counter = 0\n",
    "        self.memory = []\n",
    "        self.memCntr = 0\n",
    "        self.replace_target_cnt = replace\n",
    "        self.Q_eval = DeepQNetwork(alpha)\n",
    "        self.Q_next = DeepQNetwork(alpha)\n",
    "\n",
    "    def storeTransition(self, state, action, reward, state_):\n",
    "        if self.memCntr < self.memSize:\n",
    "            self.memory.append([state, action, reward, state_])\n",
    "        else:\n",
    "            self.memory[self.memCntr%self.memSize] = [state, action, reward, state_]\n",
    "        self.memCntr += 1\n",
    "\n",
    "    def chooseAction(self, observation):\n",
    "        rand = np.random.random()\n",
    "        actions = self.Q_eval.forward(observation)\n",
    "        if rand < 1 - self.EPSILON:\n",
    "            action = T.argmax(actions[1]).item()\n",
    "        else:\n",
    "            action = np.random.choice(self.actionSpace)\n",
    "        self.steps += 1\n",
    "        return action\n",
    "\n",
    "    def learn(self, batch_size):\n",
    "        self.Q_eval.optimizer.zero_grad()\n",
    "        if self.replace_target_cnt is not None and \\\n",
    "           self.learn_step_counter % self.replace_target_cnt == 0:\n",
    "            self.Q_next.load_state_dict(self.Q_eval.state_dict())\n",
    "\n",
    "        if self.memCntr+batch_size < self.memSize:\n",
    "            memStart = int(np.random.choice(range(self.memCntr)))\n",
    "        else:\n",
    "            memStart = int(np.random.choice(range(self.memSize-batch_size-1)))\n",
    "        miniBatch=self.memory[memStart:memStart+batch_size]\n",
    "        memory = np.array(miniBatch, dtype=object)\n",
    "\n",
    "        # convert to list because memory is an array of numpy objects\n",
    "        Qpred = self.Q_eval.forward(torch.stack(memory[:,0][:].tolist())).to(self.Q_eval.device)\n",
    "        Qnext = self.Q_next.forward(torch.stack(memory[:,3][:].tolist())).to(self.Q_eval.device)\n",
    "\n",
    "        maxA = T.argmax(Qnext, dim=1).to(self.Q_eval.device)\n",
    "        rewards = T.Tensor(list(memory[:,2])).to(self.Q_eval.device)\n",
    "        Qtarget = Qpred.clone()\n",
    "        indices = np.arange(batch_size)\n",
    "        Qtarget[indices,maxA] = rewards + self.GAMMA*T.max(Qnext[1])\n",
    "\n",
    "        if self.steps > 500:\n",
    "            if self.EPSILON - 1e-4 > self.EPS_END:\n",
    "                self.EPSILON -= 1e-4\n",
    "            else:\n",
    "                self.EPSILON = self.EPS_END\n",
    "\n",
    "        #Qpred.requires_grad_()\n",
    "        loss = self.Q_eval.loss(Qtarget, Qpred).to(self.Q_eval.device)\n",
    "        loss.backward()\n",
    "        self.Q_eval.optimizer.step()\n",
    "        self.learn_step_counter += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca9b57a-61e0-48a9-89b8-660ed636590f",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83614773-47e3-4a2a-9504-abed0090bdac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Query the data from Mongo\n",
    "# df = pd.DataFrame(mongo.find({}))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3252a4-b1e4-44f2-b784-74c9d9fc2874",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save = brain.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d13f0dc-e11b-4250-b064-1bee8d20e5f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = FingerprintEnv(df)\n",
    "brain = Agent(gamma=0.95, epsilon=1.0, alpha=0.003, maxMemorySize=5000, replace=None, actionSpace=env.possibleActions)\n",
    "bar = tqdm.tqdm(desc=\"Memory Filler\", total=5000)\n",
    "# while brain.memCntr < brain.memSize:\n",
    "#     observation = env.reset()\n",
    "#     done = False\n",
    "#     while not done:\n",
    "#         action = np.random.choice(env.possibleActions)\n",
    "#         observation_, reward, done, info = env.step(action)\n",
    "#         brain.storeTransition(observation, action, reward, observation_)\n",
    "#         observation = observation_\n",
    "#         bar.update()\n",
    "print('done initializing memory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bddbee-7ffa-4bab-a3bd-bb2bc5395634",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "brain.memory = save[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce768fa0-c020-404c-bdcf-0b7242691a0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "epsHistory = []\n",
    "fingerprints_found = {}\n",
    "numGames = 50\n",
    "batch_size=32\n",
    "# uncomment the line below to record every episode.\n",
    "# env = wrappers.Monitor(env, \"tmp/space-invaders-1\", video_callable=lambda episode_id: True, force=True)\n",
    "bar = tqdm.tqdm(desc=\"Training\", total=numGames)\n",
    "for i in range(numGames):\n",
    "\n",
    "    print(f\"Finding fp-{i+1}. Epsilon: {brain.EPSILON}\")\n",
    "    env.render()\n",
    "    epsHistory.append(brain.EPSILON)\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    score = 0\n",
    "    lastAction = 0\n",
    "    while not done:\n",
    "        # print(observation)\n",
    "        action = brain.chooseAction(observation)\n",
    "        # print(action)\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        # display(transforms.functional.to_pil_image(observation_))\n",
    "        # print(reward)\n",
    "        # print(done)\n",
    "        score += reward\n",
    "        # print(score)\n",
    "        brain.storeTransition(observation, action, reward, observation_)\n",
    "        # print(\"Stored\")\n",
    "        observation = observation_\n",
    "        brain.learn(batch_size)\n",
    "        lastAction = action\n",
    "    bar.update()\n",
    "    env.render()\n",
    "    scores.append(score)\n",
    "    fingerprints_found[f\"Training:{i+1}\"] = env.moniker_progress()\n",
    "    print(f'score: {score}')\n",
    "plt.plot(scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9274a4d4-39ca-4da2-9bc4-159597e73b67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
